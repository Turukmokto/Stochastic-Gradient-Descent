# Stochastic-Gradient-Descent

1. Implement stochastic gradient descent to solve linear regression these. Explore convergence with different batch sizes (1 - sgd, 2...n-1 - minibatch gd, n - gd from previous work).

2. Investigate the effect of preliminary data normalization (scaling) on similar dimness.

3. Explore gradient descent modifications (nesterov, momentum, adagrad, rmsprop, adam).

4. Explore the convergence of algorithms. Compare different methods for speed convergence, reliability. required small resources (volume of operational memory, number of arithmetic operations, execution time)

5. Construct the trajectory of the descent of various algorithms from the same reference point with the same accuracy. In the report, superimpose this trajectory on drawing with lines of equal level of a given function.
